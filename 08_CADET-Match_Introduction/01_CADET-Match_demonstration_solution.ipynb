{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8: CADET-Match\n",
    "\n",
    "As we have seen in the previous lesson, `CADET-Match` is a tool for estimating parameters using CADET.\n",
    "Amonst others, `CADET-Match` offers functionality for estimating parameters considering: \n",
    "* Multiple parameters\n",
    "* Multiple components\n",
    "* Per-component parameters\n",
    "* Multiple experiments\n",
    "* Multiple scores\n",
    "* Scores that apply to only a subset of the data\n",
    "* Fractionation of products\n",
    "\n",
    "\n",
    "For more information, see also:\n",
    "- **Source:** https://github.com/modsim/CADET-Match\n",
    "- **Documentation:** will soon also be published on https://cadet.github.io\n",
    "\n",
    "**In this lesson,** we will learn:\n",
    "- Fundamentals of scoring functions, search methods, and parameter transforms.\n",
    "- Setting up a first simple example with `CADET-Match`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Automatic determination of porosity and axial dispersion of a Column\n",
    "\n",
    "Again we look at the previous example where we manually tried to determine the porosity and axial dispersion of a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CADETSettings\n",
    "\n",
    "This imports all the standard libraries and provides some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CADET-Match configuration\n",
    "\n",
    "For configuring `CADET-Match`, we simply create an instance of a `Dict` from the `addict` module. \n",
    "We also again need to provide the path to the `CADET-cli`, and set the base directory for the current parameter estimation procedure (all paths are read relative to this directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "\n",
    "base_dir = Path('./').absolute()\n",
    "\n",
    "match_config = Dict()\n",
    "match_config.CADETPath = Cadet.cadet_path\n",
    "match_config.baseDir = base_dir.as_posix()\n",
    "match_config.resultsDir = 'results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reference model\n",
    "\n",
    "First, we need to model our system.\n",
    "For this purpose, we configure a `CADET` model as explained in the previous lessons.\n",
    "In this exercise, we will reuse the previous example which included an `INLET`, a `LUMPED_RATE_MODEL_WITH_PORES`, and an `OUTLET`.\n",
    "We can load a preconfigured model from the utils by calling `create_dextran_model`.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning:** \n",
    "\n",
    "In order for `CADET-Match` to work, it is important to set `SPLIT_COMPONENTS_DATA` in the [`/input/return`](https://cadet.github.io/interface/return_data.html) group to `True`.\n",
    "With this, a separate dataset for each component is created (`XXX_COMP_000`, `XXX_COMP_001`,...).\n",
    "\n",
    "</div>\n",
    "\n",
    "It is important to remember the location of the `.h5` file because we later need to add it to the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dextran_model = create_dextran_model()\n",
    "run_simulation(dextran_model, 'dextran_reference.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "From our reference model, we can now select any number parameters and add them to `CADET-Match`.\n",
    "First, we need to specify the path to the parameter in the model.\n",
    "Moreover, for every parameter, we can specify:\n",
    "\n",
    "- min: `float`\\\n",
    "  minimum value the parameter should take.\n",
    "- max: `float`\\\n",
    "  maximum value the parameter should take.\n",
    "- component: `int`\\\n",
    "  index of the component. `-1` means the parameter is independent of components.\n",
    "- bound: `int`\\\n",
    "  bound state. `-1` means the paramter is indepdent of bound states.\n",
    "- transform: `str`\\\n",
    "  one of `{'null', 'auto', 'auto_keq', 'auto_inverse', 'norm_volume_area', 'norm_volume_length', 'set_value'}`.\n",
    "    \n",
    "Again, we create another `Dict` for every parameter and specify the required data.\n",
    "For now, we will ignore the transform but we will look at them later in detail.\n",
    "Then, we add them to the parameters list of the `match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter1 = Dict()\n",
    "parameter1.location = '/input/model/unit_001/COL_DISPERSION'\n",
    "parameter1.min = 1e-10\n",
    "parameter1.max = 1e-6\n",
    "parameter1.component = -1\n",
    "parameter1.bound = -1\n",
    "parameter1.transform = 'null'\n",
    "\n",
    "parameter2 = Dict()\n",
    "parameter2.location = '/input/model/unit_001/COL_POROSITY'\n",
    "parameter2.min = 0.1\n",
    "parameter2.max = 0.9\n",
    "parameter2.component = -1\n",
    "parameter2.bound = -1\n",
    "parameter2.transform = 'null'\n",
    "\n",
    "match_config.parameters = [parameter1, parameter2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding experimental data\n",
    "\n",
    "Next, we need to specify the experimental data with which the simulation will be compared to.\n",
    "Currently, `CADET-Match` only handles `.csv` files for experimental data.\n",
    "For this demonstration, the data can be found unter `/resources/example.csv`.\n",
    "\n",
    "We also need to provide the path of the `.h5` file that has the base simulation in it.\n",
    "Furthermore, we need to specify the path of the data in the `Cadet` model that needs to match the experimental data.\n",
    "\n",
    "We create another `Dict` for every experiment and specify the required data.\n",
    "Then, we add the experiment to the `match` experiments list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1 = Dict()\n",
    "experiment1.name = 'dextran'\n",
    "experiment1.csv = '../resources/dextran_experiment.csv'\n",
    "experiment1.HDF5 = 'dextran_reference.h5'\n",
    "experiment1.isotherm = '/output/solution/unit_002/SOLUTION_OUTLET_COMP_000'\n",
    "\n",
    "match_config.experiments = [experiment1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featues and scores\n",
    "\n",
    "Scores are how the search algorithm measures if a solution is better or worse than what it had before. \n",
    "Without a good score system an optimizer can't work. \n",
    "There are many scores available and here we will discuss the most commonly used ones. \n",
    "A complete overview can be found [here](https://github.com/modsim/CADET-Match/tree/master/CADETMatch/scores).\n",
    "\n",
    "Most of these scores are based on physical understanding of the problem and it is pretty easy to add new scores.\n",
    "If there is a feature you can identify in your problem that ties to one of the variables being estimated then go ahead and make a score for it.\n",
    "Maybe for your isotherm the width of the peak or some other feature reveals important information.\n",
    "\n",
    "1. **Shape**\\\n",
    "  This is the default score used when fitting an isotherm. \n",
    "  It looks at the shape, position, and height of the peak. \n",
    "  It has a small initial penalty for the peak being out of position in time and increases as the peak gets further away. \n",
    "  This is to deal with the problem of pump delays and other experimental errors. \n",
    "  The best fit may not be and often is not the peak that is in exactly the right location but it is the one with the right shape. \n",
    "  The shape is determined by the physics which is why it is so heavily emphasied in the score system.\n",
    "\n",
    "1. **ShapeDecay**\\\n",
    "  This is the default score to use when fitting transport parameters. \n",
    "  Transport experiments are more difficult because you MUST make sure that as few errors as possible happened in the physical experiment. \n",
    "  On most AKTA type systems if there is an alarm the timer for data collection is also paused until the alarm is cleared and the system resumes. \n",
    "  his results in data that has no gap in the time. \n",
    "  However, the chemicals are still reacting and moving even if the pumps are off and data collection stops. \n",
    "  Any such error that occurs will result in the diffusion being wrong and the longer the error persists before being fixed the more wrong the estimate for diffusion will be. \n",
    "  Things like pump delays or other pauses in the system can cause problems with estimating volume or porosity. \n",
    "  Any errors that occur in transport experiments will be carried through to isotherm parameters which harms scaleup later.\n",
    "  \n",
    "1. **ShapeFront**\\\n",
    "  This score looks at only the front of the peak. \n",
    "  It is designed to use with a breakthrough that just levels off and does not go back down.\n",
    "\n",
    "1. **ShapeBack**\\\n",
    "  This score looks at only the back of the peak. It is designed for situations that start at a high concentration and drops over time.\n",
    "\n",
    "1. **DextranShape**\\\n",
    "  Dextran is a nice tracer overall but it has non ideal behavior in most systems. \n",
    "  You normally see this in strange tailing behavior. \n",
    "  What this means is that only the front side of the peak can be used. \n",
    "  This score specifically looks only at the front side of the peak.\n",
    "  \n",
    "1. **DextranSSE**\\\n",
    "  This works like DextranShape and cuts out just the front part of the peak but uses SSE for the score. **This score can not be combined with any of the other scores since it does not range from 0 to 1.**\n",
    "\n",
    "1. **fractionationSlide**\\\n",
    "  If you have a fractionation proble use this score. Internally it is a little complex. After each simulation for each species it numerically slides the chromatogram back and forward in time and refactionates at each time to find which location best matches the experimental data to find the time difference between the synthetic and experimental peak along with a shape comparison at that time.\n",
    "\n",
    "1. **SSE**\\\n",
    "  This is the Sum of Squared Errors. **This score can not be combined with any of the other scores since it does not range from 0 to 1.**\n",
    "\n",
    "1. **AbsoluteHeight** and **AbsoluteTime**\\\n",
    "  These scores are needed by the MCMC system. They are absolute measurements of height and time since the beginning of the simulation. They are used to capture the impact of errors that have an assymetric impact on the peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a feature in CADET-Match, create a `Dict` and add it to the list of features of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = Dict()\n",
    "feature1.name = \"Pulse\"\n",
    "feature1.type = 'SSE'\n",
    "\n",
    "experiment1.features = [feature1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search MethodMethodMethod\n",
    "\n",
    "\n",
    "There are 5 main search methods and only 3 of them are normally used.\n",
    "\n",
    "1. **NSGA3**\\\n",
    "  This is the default evolutionary algorithm and the best choice to use when you don't know anything about your search space.\n",
    "\n",
    "1. **Multistart**\\\n",
    "  This is a multistart gradient descent algorithm. If your problem can be solved with gradient descent then this can be a very good choice. It creates an initial population the same way that NSGA3 does so that you don't have to worry about starting point. \n",
    "\n",
    "1. **GraphSpace**\\\n",
    "  This is not for optimization and instead it is just for sampling the space. If you have a new problem and you have no idea what the range of any of the parameters are you can use this with a very large population and it will just evaluate the entire starting population and then terminate. This allows you to figure out what are plausiable ranges for your variables before using one of the search algorithms. When dealing with many variables it is still possible to miss regions of viable solutions inside large non-viable regions but in practice this has rarely been a problem.\n",
    "\n",
    "1. **MCMC**\\\n",
    "  This performans the error modeling and sampling. It is not an optimization algorithm and while it will probably find the optimum value while it runs it is very slow to do so and won't stop even if it finds the optimal value. This should also not be used directly. Normally NSGA3 is used and an option is set to continue with MCMC after NSGA3 has completed. This allows MCMC to be seeded with the best values from NSGA3 which dramatically improves convergence. If you have many variables and you run this directly it may never converge since it may never find high probability regions, much less sample from them effectively. \n",
    " \n",
    "1. **Gradient**\\\n",
    "  This is a single start gradient descent algorithm. In order to work you need to give a good enough starting point. Generally it is advisable to use Multistart instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this case,** we will use the 'NSGA3' search method.\n",
    "\n",
    "It also enables us to set some optimizer parameters:\n",
    "\n",
    "- population: `int`\\\n",
    "  Number of individuals that are evaluated per generation (good default: 100)\n",
    "- stallGenerations: `int`\\\n",
    "  Number of generations without progress before the search is terminated (good default: 10-50)\n",
    "- finalGradRefinement: `Bool`\\\n",
    "  If `True`, run gradient descent on the final values to refine them further\n",
    "- gradVector: `Bool`\\\n",
    "  Setting for gradient refinement. Use `True` to have SSE for refinement and `False` to use the score for gradient refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_config.searchMethod = 'NSGA3'\n",
    "match_config.population = 12\n",
    "match_config.stallGenerations = 10\n",
    "match_config.finalGradRefinement = True\n",
    "match_config.gradVector = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the config and running the optimization\n",
    "\n",
    "We save the match config as a `.json` file by calling the `json.dump()` method.\n",
    "\n",
    "Most of the time, `CADET-Match` would be started from a command line on a powerful server.\n",
    "However, to start the parameter estimation in a juypter notebook, we can import the `Match` class from the `CADETMatch.jupyter` submodule.\n",
    "All of the same results are generated as when the matching software is run directly on the command line and the full results can still be viewed.\n",
    "We create an instance by passing the path of the config that we just saved to the `__init__` function of the `Match` class and start the optimization by calling the `start_sim` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from CADETMatch.jupyter import Match\n",
    "\n",
    "match_config_file = base_dir / 'dextran.json'\n",
    "\n",
    "with open(match_config_file.as_posix(), 'w') as json_file:\n",
    "    json.dump(match_config.to_dict(), json_file, indent='\\t')\n",
    "\n",
    "match = Match(match_config_file)\n",
    "match.start_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "The system creates a number of sub directories and depending on what settings are in the JSON file some will be empty.\n",
    "\n",
    "The import ones are\n",
    "\n",
    "1. **log**\\\n",
    "  This contains all the log files from the system. If something goes wrong the information is usually here to find the problem and fix it. This is especialy useful for us to help with a problem.\n",
    "2. **meta**\\\n",
    "  This contains all the results on the pareto front of meta scores. Your best results will be in this folder. There are simulations, images and results in csv and xlsx format to view.\n",
    "3. **progress**\\\n",
    "  This folder watches the general progress of the algorithm. In this folder you can see if your variables are identifiable or the progress the search method is making.\n",
    "4. **space**\\\n",
    "  This folder has plots for what the search space looks like. This is especially useful for debugging. Based on graphs in this folder you can expand or contract the search range for your variables which can improve the fits.\n",
    "  \n",
    "There are also some important items in this folder\n",
    "1. `error.csv`\\\n",
    "  If any simulation fails it will be recorded here\n",
    "2. `json file`\\\n",
    "  This is the JSON used to create this simulation. It is a copy of the JSON file you started the matching process with\n",
    "3. `progress.csv`\\\n",
    "  This file contains some information on algorithm progress. Of special interest are the last two columns that show the last generation where a new item was added to the pareto front of meta scores and how many generations in a row that progress has been made. If enough generations pass without progress the search will terminate.\n",
    "4. `result.h5`\\\n",
    "  This is an HDF5 file that contains enormous information about the system. It has the entire population, the scores for all items, and other information. With HDFview you can easily view these files and with python you can read any information you want from these files.\n",
    "5. `csv file`\\\n",
    "  There is one last csv file that is named based on the JSON configuration file and it shows every simulation the system has run along with the scores for that simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create corner plots\n",
    "These are frequency plots of each variable in your system along with their correlations. What you are looking for is a single peak and this indicates that your variable is identifiable. The peak may be more or less narrow depending on the ranges you gave to the optimizer. If you use an early stopping criteria these plots probably won't be accurate. What is happening is that as the system converges it searches around the best found values to try and find better ones. This means that there are many similar values and so the frequency goes up which shows up as a peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match.plot_corner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best plots\n",
    "\n",
    "This draws a selection of items from the meta folder. \n",
    "It will look in the folder and find the highest Product Root Score, Min Score and Mean Score and the lowest SSE and plot all of those entries. \n",
    "There may still be other items in those folders which are compromises between these scores. \n",
    "The plot will show the ID of the best found item, by what measurements it is the best and what its paramters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T08:53:13.417029Z",
     "start_time": "2019-10-22T08:53:13.367029Z"
    }
   },
   "outputs": [],
   "source": [
    "match.plot_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable transforms\n",
    "\n",
    "Search algorithms don't work well when the parameters have very different ranges.\n",
    "For example if one parmeter varies from $1 \\cdot 10^{-5}$ to $1 \\cdot 10^{-10}$ and the other varies from $1$ to $10$.\n",
    "Another problem is when parameters are tightly coupled together but presented to the optimizer as indepdent variables. \n",
    "For example, the adsorption and desorption rates kA and kD are coupled together but given as two different variables while they do not freely vary from each other. \n",
    "To solve this problem variable transforms are available. \n",
    "A complete overview of all transforms can be found [here](https://github.com/modsim/CADET-Match/tree/master/CADETMatch/transform).\n",
    "\n",
    "\n",
    "1. **auto**\\\n",
    "  This is a transform that normalizes a variable from 0 to 1. \n",
    "  If max/min < 1e-3 a lot transform is used otherwise a linear transform is used.\n",
    "\n",
    "1. **auto_keq**\\\n",
    "  This transform turns ka, kd in ka and keq=ka/kd and then takes a normalizes from 0 to 1. \n",
    "  If maxKA/minKA > 1e3 a log transform is used otherwise a linear transform is used. \n",
    "  If maxKEQ/minKEQ > 1e3 a log transform is used otherwise a linear transform is used.\n",
    "  \n",
    "1. **auto_inverse**\\\n",
    "  This transform normalizes from 0 to 1 and uses a log transform for max/min > 1e3 otherwise a linear transform.\n",
    "  The variable used is 1/x and this is designed for estimating film diffusion and pore diffusion with a non-binding pulse. \n",
    "  Applying this transform to both film diffusion and pore diffusion decouples the variables to some extent and improved parameter estimation and MCMC.\n",
    "  \n",
    "1. **norm_volume_area**\\\n",
    "  This transform and the following one where created to solve the same common problem. \n",
    "  When fitting bypass data and you model the tubing you are fitting the length, crosssectional area and dispersion of the tubing. \n",
    "  Dispersion though is tied to the length such that a shorter length can just be compensated for with a higher diffusion. \n",
    "  Basically we have fewer degrees of freedom that we are fitting with and since this is not a fluid dynamics model it is not physically accurate. \n",
    "  This transformation couples the length and area together through the volume since volume is an identfiable quantity (since water is incompressible). \n",
    "  We don't actually care much about the parameters for the tubing, it is just there to fill in for the real tubing so that fewer errors are carried into the column. \n",
    "  Use this version if it is easier to measure the diameter of the tubing and use the other transform if measuring the length is easier. \n",
    "  It just depends on your experimental setup.\n",
    "\n",
    "1. **norm_volume_length**\\\n",
    "  Use this transform if measuring the length accurately is easier than measuring the diameter of the tubing. \n",
    "  If you are lumping together all the tubing in front of the column or after the column together and you have multiple diameters it doesn't matter which of these transforms you choose.\n",
    "\n",
    "1. **set_value**\\\n",
    "  This is a different kind of transform. \n",
    "  What it allows you to do is copy a value from place to another. \n",
    "  This is important if you are estimating axial dispersion for identical tubing in front of and behind the column. \n",
    "  You want the same axial dispersion for both pieces of tubing but you only want to estimate one variable. \n",
    "  With this you can estimate axial dispersion for the first piece of tubing and then copy the value over to the other piece of tubing. \n",
    "  This is also required if you want to estimate SMA_LAMDA. Whenever SMA_LAMBDA change the initial bound salt concentration also much change or electronuetrality would be violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will look again at the previous example. \n",
    "This time, we will tell `CADET-Match` to normalize the variable using the `auto` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter1 = Dict()\n",
    "parameter1.location = \"/input/model/unit_001/COL_DISPERSION\"\n",
    "parameter1.component = -1\n",
    "parameter1.bound = -1\n",
    "parameter1.min = 1e-10\n",
    "parameter1.max = 1e-6\n",
    "parameter1.transform = 'auto'\n",
    "\n",
    "parameter2 = Dict()\n",
    "parameter2.location = \"/input/model/unit_001/COL_POROSITY\"\n",
    "parameter2.component = -1\n",
    "parameter2.bound = -1\n",
    "parameter2.min = 0.2\n",
    "parameter2.max = 0.5\n",
    "parameter2.transform = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = Dict()\n",
    "match.CADETPath = cadet_path.as_posix()\n",
    "match.baseDir = example_dir_2.as_posix()\n",
    "match.resultsDir = 'fit_grad'\n",
    "match.searchMethod = 'Gradient'\n",
    "match.seeds = [(ka, kd, qmax), ]\n",
    "match.gradVector = True\n",
    "\n",
    "#variable transforms\n",
    "parameter1 = Dict()\n",
    "\n",
    "#use the norm_keq transform\n",
    "parameter1.transform = 'norm_keq'\n",
    "\n",
    "#this is the component, in this case we want component 0\n",
    "parameter1.component = 0\n",
    "\n",
    "#this is the bound state we are looking at, this is usually 0 unless you are using multi-state binding\n",
    "parameter1.bound = 0\n",
    "\n",
    "#norm_keq location takes a list with the path to ka and the path to kd in the file\n",
    "parameter1.location = [\"/input/model/unit_001/adsorption/MCL_KA\", \"/input/model/unit_001/adsorption/MCL_KD\"]\n",
    "\n",
    "#minimm and maximum values for the search\n",
    "parameter1.minKA = 1e-5\n",
    "parameter1.maxKA = 1e5\n",
    "parameter1.minKEQ = 1e-4\n",
    "parameter1.maxKEQ = 1e4\n",
    "\n",
    "parameter2 = Dict()\n",
    "parameter2.transform = 'norm'\n",
    "parameter2.component = 0\n",
    "parameter2.bound = 0\n",
    "parameter2.location = \"/input/model/unit_001/adsorption/MCL_QMAX\"\n",
    "parameter2.min = 1\n",
    "parameter2.max = 20\n",
    "\n",
    "match.parameters = [parameter1, parameter2, ]\n",
    "\n",
    "experiment1 = Dict()\n",
    "experiment1.csv = \"example.csv\"\n",
    "experiment1.isotherm = \"/output/solution/unit_002/SOLUTION_OUTLET_COMP_000\"\n",
    "experiment1.HDF5 = \"example.h5\"\n",
    "experiment1.name = \"main\"\n",
    "\n",
    "feature1 = Dict()\n",
    "feature1.name = \"Pulse\"\n",
    "feature1.type = 'SSE'\n",
    "\n",
    "experiment1.features = [feature1,]\n",
    "\n",
    "match.experiments = [experiment1,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "401.997px",
    "width": "424.983px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
